# PIPELINE DEFINITION
# Name: fracture-classification-v1-1
# Inputs:
#    batch_size: str [Default: '10']
#    experiment_name: str [Default: 'default']
#    nr_of_epochs: int [Default: 5.0]
#    run_name: str [Default: 'default']
components:
  comp-run-a-file:
    executorLabel: exec-run-a-file
  comp-run-a-file-2:
    executorLabel: exec-run-a-file-2
    inputDefinitions:
      parameters:
        batch_size:
          defaultValue: '10'
          isOptional: true
          parameterType: STRING
        nr_of_epochs:
          defaultValue: 5.0
          isOptional: true
          parameterType: NUMBER_INTEGER
  comp-run-a-file-3:
    executorLabel: exec-run-a-file-3
    inputDefinitions:
      parameters:
        experiment_name:
          defaultValue: default
          isOptional: true
          parameterType: STRING
        run_name:
          defaultValue: default
          isOptional: true
          parameterType: STRING
deploymentSpec:
  executors:
    exec-run-a-file:
      container:
        args:
        - 'sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"

          sh -c "echo ''Downloading file:///opt/app-root/bin/utils/bootstrapper.py''
          && curl --fail -H ''Cache-Control: no-cache'' -L file:///opt/app-root/bin/utils/bootstrapper.py
          --output bootstrapper.py"

          sh -c "echo ''Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt''
          && curl --fail -H ''Cache-Control: no-cache'' -L file:///opt/app-root/bin/utils/requirements-elyra.txt
          --output requirements-elyra.txt"

          sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt
          && python3 bootstrapper.py --pipeline-name ''fracture-classification-v1.1''
          --cos-endpoint ''https://poc-minio-api-route-bram-terlouw-dev.apps.sandbox-m2.ll9k.p1.openshiftapps.com''
          --cos-bucket ''ds-pipelines'' --cos-directory ''fracture-classification-v1.1-0822093129''
          --cos-dependencies-archive ''Load_Data-8ded5050-3e9b-4c2c-b6b6-732871b5c896.tar.gz''
          --file ''Beroepsproduct/Models/Classification model/Load_Data.ipynb'' --outputs
          ''images_zipped.zip'' "

          '
        command:
        - sh
        - -c
        env:
        - name: AWS_S3_ENDPOINT
          value: http://poc-minio-service:9000
        - name: AWS_S3_BUCKET
          value: images
        - name: AWS_DEFAULT_REGION
          value: eu-west-1
        - name: ELYRA_RUNTIME_ENV
          value: kfp
        - name: ELYRA_ENABLE_PIPELINE_INFO
          value: 'True'
        - name: ELYRA_WRITABLE_CONTAINER_DIR
          value: /tmp
        - name: ELYRA_RUN_NAME
          value: '{{workflow.uid}}'
        image: quay.io/modh/runtime-images@sha256:90e394f5a379c24176b1efee6c84b83866314cafd539a66cd58544f24def84f9
        resources:
          cpuRequest: 1.0
    exec-run-a-file-2:
      container:
        args:
        - 'nr_of_epochs="$0"

          batch_size="$1"

          sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"

          sh -c "echo ''Downloading file:///opt/app-root/bin/utils/bootstrapper.py''
          && curl --fail -H ''Cache-Control: no-cache'' -L file:///opt/app-root/bin/utils/bootstrapper.py
          --output bootstrapper.py"

          sh -c "echo ''Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt''
          && curl --fail -H ''Cache-Control: no-cache'' -L file:///opt/app-root/bin/utils/requirements-elyra.txt
          --output requirements-elyra.txt"

          sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt
          && python3 bootstrapper.py --pipeline-name ''fracture-classification-v1.1''
          --cos-endpoint ''https://poc-minio-api-route-bram-terlouw-dev.apps.sandbox-m2.ll9k.p1.openshiftapps.com''
          --cos-bucket ''ds-pipelines'' --cos-directory ''fracture-classification-v1.1-0822093129''
          --cos-dependencies-archive ''Train_Model-ab6c943e-958c-4853-9abf-b35da2da9c0c.tar.gz''
          --file ''Beroepsproduct/Models/Classification model/Train_Model.ipynb''
          --inputs ''images_zipped.zip'' --outputs ''fracture_model.h5'' --pipeline-parameters
          ''nr_of_epochs=$nr_of_epochs;batch_size=$batch_size'' --parameter-pass-method
          ''env'' "

          '
        - '{{$.inputs.parameters[''nr_of_epochs'']}}'
        - '{{$.inputs.parameters[''batch_size'']}}'
        command:
        - sh
        - -c
        env:
        - name: ELYRA_RUNTIME_ENV
          value: kfp
        - name: ELYRA_ENABLE_PIPELINE_INFO
          value: 'True'
        - name: ELYRA_WRITABLE_CONTAINER_DIR
          value: /tmp
        - name: ELYRA_RUN_NAME
          value: '{{workflow.uid}}'
        image: quay.io/modh/runtime-images@sha256:90e394f5a379c24176b1efee6c84b83866314cafd539a66cd58544f24def84f9
        resources:
          cpuRequest: 1.0
          memoryLimit: 8.0
          memoryRequest: 8.0
    exec-run-a-file-3:
      container:
        args:
        - 'experiment_name="$0"

          run_name="$1"

          sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"

          sh -c "echo ''Downloading file:///opt/app-root/bin/utils/bootstrapper.py''
          && curl --fail -H ''Cache-Control: no-cache'' -L file:///opt/app-root/bin/utils/bootstrapper.py
          --output bootstrapper.py"

          sh -c "echo ''Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt''
          && curl --fail -H ''Cache-Control: no-cache'' -L file:///opt/app-root/bin/utils/requirements-elyra.txt
          --output requirements-elyra.txt"

          sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt
          && python3 bootstrapper.py --pipeline-name ''fracture-classification-v1.1''
          --cos-endpoint ''https://poc-minio-api-route-bram-terlouw-dev.apps.sandbox-m2.ll9k.p1.openshiftapps.com''
          --cos-bucket ''ds-pipelines'' --cos-directory ''fracture-classification-v1.1-0822093129''
          --cos-dependencies-archive ''Save_Model-faf8533b-ee1d-4bd3-86d9-0bbfd2003aa1.tar.gz''
          --file ''Beroepsproduct/Models/Classification model/Save_Model.ipynb'' --inputs
          ''images_zipped.zip;fracture_model.h5'' --pipeline-parameters ''experiment_name=$experiment_name;run_name=$run_name''
          --parameter-pass-method ''env'' "

          '
        - '{{$.inputs.parameters[''experiment_name'']}}'
        - '{{$.inputs.parameters[''run_name'']}}'
        command:
        - sh
        - -c
        env:
        - name: AWS_S3_ENDPOINT
          value: http://poc-minio-service:9000
        - name: AWS_DEFAULT_REGION
          value: eu-west-1
        - name: ELYRA_RUNTIME_ENV
          value: kfp
        - name: ELYRA_ENABLE_PIPELINE_INFO
          value: 'True'
        - name: ELYRA_WRITABLE_CONTAINER_DIR
          value: /tmp
        - name: ELYRA_RUN_NAME
          value: '{{workflow.uid}}'
        image: quay.io/modh/runtime-images@sha256:90e394f5a379c24176b1efee6c84b83866314cafd539a66cd58544f24def84f9
        resources:
          cpuRequest: 1.0
          memoryLimit: 8.0
          memoryRequest: 8.0
pipelineInfo:
  name: fracture-classification-v1-1
root:
  dag:
    tasks:
      run-a-file:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-a-file
        taskInfo:
          name: Load_Data
      run-a-file-2:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-a-file-2
        dependentTasks:
        - run-a-file
        inputs:
          parameters:
            batch_size:
              componentInputParameter: batch_size
            nr_of_epochs:
              componentInputParameter: nr_of_epochs
        taskInfo:
          name: Train_Model
      run-a-file-3:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-a-file-3
        dependentTasks:
        - run-a-file-2
        inputs:
          parameters:
            experiment_name:
              componentInputParameter: experiment_name
            run_name:
              componentInputParameter: run_name
        taskInfo:
          name: Save_Model
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: '10'
        isOptional: true
        parameterType: STRING
      experiment_name:
        defaultValue: default
        isOptional: true
        parameterType: STRING
      nr_of_epochs:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      run_name:
        defaultValue: default
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.5.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-run-a-file:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            secretName: dashboard-dspa-secret
        exec-run-a-file-2:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            secretName: dashboard-dspa-secret
        exec-run-a-file-3:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            secretName: dashboard-dspa-secret
